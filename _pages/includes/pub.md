# üìù Publications 
## ![](https://img.shields.io/badge/-%23FF6600?style=flat-square&logo=GraphQL&logoColor=white) Knowledge Graph

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2025</div><img src='images/kgrsurvey.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
[A Survey of Task-Oriented Knowledge Graph Reasoning: Status, Applications, and Prospects](https://arxiv.org/abs/2506.11012) \\
**Guanglin Niu**, Bo Li, Yangguang Lin \\
_Arxiv, 2025_

- This survey aims to highlight key research trends and outline promising future directions of Knowledge Graph Reasoning (KGR) from the perspective of primary reasoning tasks, downstream applications, and challenging tasks.
- We explore advanced techniques, such as large language models, and their impact on KGR. 
- Our work is promoted by some media, such as [‰∫∫Â∑•Êô∫ËÉΩÈÅá‰∏äÁü•ËØÜÂõæË∞±](https://mp.weixin.qq.com/s/0hdBi_BC_WzmnZJrWHDqGQ).

üìÉ[**Paper**](https://arxiv.org/pdf/2506.11012)     üîß[**Project**](https://github.com/ngl567/KGR-Survey) ![img](https://img.shields.io/github/stars/ngl567/KGR-Survey?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">DASFAA 2025</div><img src='images/dhns.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Diffusion-based Hierarchical Negative Sampling for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2501.15393) \\
**Guanglin Niu**, Xiaowei Zhang \\
_The 30th International Conference on Database Systems for Advanced Applications (DASFAA), 2025_

- DHNS is the first to leverage the diffusion model‚Äôs capabilities within the context of multi-modal knowledge graph for negative sampling.

üìÉ[**Paper**](https://arxiv.org/pdf/2501.15393)     üíæ[**Code**](https://github.com/ngl567/DHNS) ![img](https://img.shields.io/github/stars/ngl567/DHNS?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2023</div><img src='images/lcge.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Logic and Commonsense-Guided Temporal Knowledge Graph Completion](https://ojs.aaai.org/index.php/AAAI/article/view/25579) \\
**Guanglin Niu**, Bo Li \\
_Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2023_

  - This work LCGE is the first to introduce temporal rules into temporal knowledge graph completion models.
  - LCGE models each event from the perspectives of both the time-sensitive representation and the commonsense.
  - Our work is promoted by some forums, such as [AI TimeÈùíÂπ¥ÁßëÂ≠¶ÂÆ∂ËÆ∫Âùõ](https://mp.weixin.qq.com/s/GP_S9U4EWJD0JGZcdJO3lg).

üìÉ[**Paper**](https://ojs.aaai.org/index.php/AAAI/article/view/25579)     üíæ[**Code**](https://github.com/ngl567/LCGE) ![img](https://img.shields.io/github/stars/ngl567/LCGE?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2022</div><img src='images/cake.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion](https://aclanthology.org/2022.acl-long.205) <strong><span class='show_paper_citations' data='ibL7gEcAAAAJ:3fE2CSJIrl8C'></span></strong> \\
**Guanglin Niu**, Bo Li, Yongfei Zhang, Shiliang Pu \\
_Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (ACL), 2022_

  - This work CAKE is the first to propose a scalable knowledge graph completion framework to predict entities in a joint commonsense and fact-driven fashion.
  - CAKE generates commonsense automatically for negative sampling and multi-view link prediction.
  - Our work is promoted by several media and forums, such as [AI Time ËßÜÈ¢ë](https://www.bilibili.com/video/BV1Q44y1g78Z/) \| [AI Time Ëß£ËØª](https://mp.weixin.qq.com/s/xQ625k_2kYXerZtO6M8mGg)„ÄÅ[‰∏ìÁü•](https://www.zhuanzhi.ai/document/5648511d67d6e512eb3521ac47d763d4)„ÄÅ[Êô∫Ê∫êÁ§æÂå∫](https://hub.baai.ac.cn/view/19366)„ÄÅ[ÂºÄÊîæÁü•ËØÜÂõæË∞±](https://mp.weixin.qq.com/s/1wVS2aJd6ddyPkvZHx3Lrw)„ÄÅ[AMiner](https://www.aminer.cn/research_report/627c81397cb68b460fb6063d).

üìÉ[**Paper**](https://aclanthology.org/2022.acl-long.205.pdf)     üíæ[**Code**](https://github.com/ngl567/CAKE) ![img](https://img.shields.io/github/stars/ngl567/CAKE?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">SIGIR 2021</div><img src='images/gana.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Relational Learning with Gated and Attentive Neighbor Aggregator for Few-Shot Knowledge Graph Completion](https://dl.acm.org/doi/10.1145/3404835.3462925) <strong><span class='show_paper_citations' data='ibL7gEcAAAAJ:WF5omc3nYNoC'></span></strong> \\
**Guanglin Niu**, Yang Li, Chengguang Tang, Ruiying Geng, Jian Dai, Qiao Liu, Hao Wang, Jian Sun, Fei Huang, Luo Si \\
_Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2021_

  - This approach GANA is the first to propose a gated and attentive neighbor aggregator to capture the most valuable contextual semantics of a relation.
  - GANA is one of the most representative models and always selected as the baseline on few-show knowledge graph completion tasks.
  - This work was conducted in collaboration with Qwen team. Our work is promoted by some media and forums, such as [‰∏ìÁü•](https://www.zhuanzhi.ai/document/01403034427fa0520e958ee1fe4afc56).

üìÉ[**Paper**](https://arxiv.org/pdf/2104.13095)     üíæ[**Code**](https://github.com/ngl567/GANA-FewShotKGC) ![img](https://img.shields.io/github/stars/ngl567/GANA-FewShotKGC?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2020</div><img src='images/rpje.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Rule-Guided Compositional Representation Learning on Knowledge Graphs](https://ojs.aaai.org//index.php/AAAI/article/view/5687)
<strong><span class='show_paper_citations' data='ibL7gEcAAAAJ:IjCSPb-OGe4C'></span></strong> \\
**Guanglin Niu**, Yongfei Zhang, Bo Li, Peng Cui, Si Liu, Jingyang Li and Xiaowei Zhang \\
_Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2020_

  - This work RPJE is the first attempt to integrate logic rules with paths for KG embedding, balancing the explainability and the generalization.
  - Our work is promoted by several media and forums, such as [ÂºÄÊîæÁü•ËØÜÂõæË∞±](https://mp.weixin.qq.com/s/tsXKwgbd2Z0XZcZZD2wcwQ)„ÄÅ[Èõ∑ÈîãÁΩë](https://www.leiphone.com/news/201912/5yfuCAlZlbFDypnH.html)„ÄÅ[SAAI](https://zhuanlan.zhihu.com/p/137519588)„ÄÅ[MLNLP](https://www.bilibili.com/video/BV1zV4y1V7j4/). Particularly, RPJE was recognized as one of the representative studies of neuro-symbolic KG reasoning at ([CCKS 2021](https://event-cdn.baai.ac.cn/live/20211228-01/Session4.mp4)).

üìÉ[**Paper**](https://ojs.aaai.org//index.php/AAAI/article/view/5687)     üíæ[**Code**](https://github.com/ngl567/RPJE) ![img](https://img.shields.io/github/stars/ngl567/RPJE?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2020 Findings</div><img src='images/autoeter.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[AutoETER: Automated Entity Type Representation with Relation-Aware Attention for Knowledge Graph Embedding](https://www.aclweb.org/anthology/2020.findings-emnlp.105/)
<strong><span class='show_paper_citations' data='ibL7gEcAAAAJ:YsMSGLbcyi4C'></span></strong> \\
**Guanglin Niu**, Yongfei Zhang, Bo Li, Shiliang Pu and Jingyang Li \\
_Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings (EMNLP Findings), 2020_

  - This work is the first to automatically learn the embeddings of entity types to enrich the general features of entities without explicit type information.
  - Our work is promoted by some media, such as [AI Time](https://www.bilibili.com/video/BV1Q44y1g78Z/)„ÄÅ[SFFAI](https://www.bilibili.com/video/av590645807/)„ÄÅ[‰∏ìÁü•](https://mp.weixin.qq.com/s/bU6Y42250GLmXiRzi8N1Wg).

üìÉ[**Paper**](https://aclanthology.org/2020.findings-emnlp.105.pdf)     üíæ[**Code**](https://github.com/ngl567/AutoETER) ![img](https://img.shields.io/github/stars/ngl567/AutoETER?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">COLING 2022</div><img src='images/enginekg.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Perform like an Engine: A Closed-Loop Neural-Symbolic Learning Framework for Knowledge Graph Inference](https://aclanthology.org/2022.coling-1.119/)\\
**Guanglin Niu**, Bo Li, Yongfei Zhang, Shiliang Pu \\
_Proceedings of the 29th International Conference on Computational Linguistics (COLING), 2022_

  - EngineKG performs like a four-stroke engine in a closed-loop neural-symbolic learning framework with embedding-based rule learning and rule-enhanced knowledge graph embedding. 
  - Our work is promoted by some media and forums, such as [MLNLP Talk](https://www.bilibili.com/video/BV1zV4y1V7j4/).

üìÉ[**Paper**](https://aclanthology.org/2022.coling-1.119.pdf)     üíæ[**Code**](https://github.com/ngl567/EngineKG)
</div>
</div>


- ![IEEE Transactions on Big Data 2025](https://img.shields.io/badge/IEEE_TBD_2025-00369F?style=flat) [A Pluggable Common Sense-Enhanced Framework for Knowledge Graph Completion](https://arxiv.org/pdf/2410.04488), **Guanglin Niu**, Bo Li, Siling Feng.
- ![ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ 2024](https://img.shields.io/badge/ËÆ°ÁÆóÊú∫ÁßëÂ≠¶_2024-00369F?style=flat&logo=ËÆ°ÁÆóÊú∫ÁßëÂ≠¶) [Èù¢ÂêëÂÖ≥Á≥ªÁâπÊÄßÂª∫Ê®°ÁöÑÁü•ËØÜÂõæË∞±Ë°®Á§∫Â≠¶‰π†Á†îÁ©∂ÁªºËø∞](https://www.jsjkx.com/CN/article/openArticlePDF.jsp?id=22673), **Guanglin Niu**, Zhen Lin.
- ![Arxiv 2024](https://img.shields.io/badge/Arxiv_2024-00369F?style=flat) [Knowledge Graph Embeddings: A Comprehensive Survey on Capturing Relation Properties](https://arxiv.org/pdf/2410.14733), **Guanglin Niu**. This paper is a modified English version of our article already published in Computer Science journal (in Chinese), released to facilitate communication among international researchers in the relevant fields.
- ![Neurocomputing 2022](https://img.shields.io/badge/Neurocomputing_2022-00369F?style=flat) [Joint Semantics and Data-Driven Path Representation for Knowledge Graph Reasoning](https://www.sciencedirect.com/science/article/abs/pii/S0925231222001515), **Guanglin Niu**, Bo Li, Yongfei Zhang, et al.
- ![ACL 2022](https://img.shields.io/badge/ACL_2021-00369F?style=flat) [Entity Concept Enhanced Few-Shot Relation Extraction](https://aclanthology.org/2021.acl-short.124/), Shan Yang, Yongfei Zhang, **Guanglin Niu**, et al. <strong><span class='show_paper_citations' data='ibL7gEcAAAAJ:ufrVoPGSRksC'></span></strong> \| [![](https://img.shields.io/github/stars/LittleGuoKe/ConceptFERE?style=social&label=Code+Stars)](https://github.com/LittleGuoKe/ConceptFERE)
- ![Arxiv 2021](https://img.shields.io/badge/Arxiv_2021-00369F?style=flat) [Path-Enhanced Multi-Relational Question Answering with Knowledge Graph Embeddings](https://arxiv.org/pdf/2110.15622.pdf), **Guanglin Niu**, Yang Li, Chengguang Tang, et al.


## üñºÔ∏è Computer Vision
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/pose2id.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization](https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_From_Poses_to_Identity_Training-Free_Person_Re-Identification_via_Feature_Centralization_CVPR_2025_paper.html) \\
Chao Yuan, Guiwei Zhang, Changxiao Ma, Tianyi Zhang, **Guanglin Niu**(Corresponding Author) \\
_Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025_

- Pose2ID is a Training-Free Feature Centralization framework that can be directly applied to different ReID tasks and models, even an ImageNet pre-trained model without ReID training.
- The developed Identity-Guided Pedestrian Generation paradigm leverages identity features to generate high-quality images of the same identity in different poses to achieve feature centralization.

üìÉ[**Paper**](https://openaccess.thecvf.com/content/CVPR2025/papers/Yuan_From_Poses_to_Identity_Training-Free_Person_Re-Identification_via_Feature_Centralization_CVPR_2025_paper.pdf)     üíæ[**Code**](https://github.com/yuanc3/Pose2ID) ![img](https://img.shields.io/github/stars/yuanc3/Pose2ID?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2025</div><img src='images/hsgl.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID](https://arxiv.org/abs/2507.11119) \\
Hankun Liu, Yujian Zhao, **Guanglin Niu**(Corresponding Author) \\
_Proceedings of the 33rd ACM International Conference on Multimedia (ACM MM), 2025_

- HSGL is a novel multimodal-guided Hard Sample Generation and Learning framework, which is the first effort to unify textual and visual modalities to explicitly define, generate, and optimize hard samples within a unified paradigm.
- We propose a hardness-aware optimization strategy that adjusts feature distances based on textual semantic labels, encouraging the separation of hard positives and drawing hard negatives closer in the embedding space to enhance the model's discriminative capability and robustness to hard samples.

üìÉ[**Paper**](https://arxiv.org/pdf/2507.11119)     üíæ[**Code**](https://github.com/undooo/TryHarder-ACMMM25) ![img](https://img.shields.io/github/stars/undooo/TryHarder-ACMMM25?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/camel.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CAMEL: CAusal Motion Enhancement tailored for Lifting Text-driven Video Editing](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_CAMEL_CAusal_Motion_Enhancement_Tailored_for_Lifting_Text-driven_Video_Editing_CVPR_2024_paper.html) \\
Guiwei Zhang, Tianyu Zhang, **Guanglin Niu**(Corresponding Author), Zichang Tan, Yalong Bai, Qing Yang \\
_Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024_

- CAMEL develops a causal motion-enhanced attention mechanism to enhance the motion coherence of latent representations while preserving content generalization to creative textual scenarios.

üìÉ[**Paper**](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_CAMEL_CAusal_Motion_Enhancement_Tailored_for_Lifting_Text-driven_Video_Editing_CVPR_2024_paper.pdf)     üíæ[**Code**](https://github.com/zhangguiwei610/CAMEL) ![img](https://img.shields.io/github/stars/zhangguiwei610/CAMEL?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2025</div><img src='images/ifd.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Identity-aware Feature Decoupling Learning for Clothing-change Person Re-identification](https://arxiv.org/pdf/2501.05851) \\
Haoxuan Xu, Bo Li, **Guanglin Niu**(Corresponding Author) \\
_2025 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2025_

- IFD is the first to propose a dual-stream identity-attention model that effectively compels the network to focus comprehensively on the regions containing distinctive identity information.

üìÉ[**Paper**](https://arxiv.org/pdf/2501.05851)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICME 2025</div><img src='images/ccup.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CCUP: A Controllable Synthetic Data Generation Pipeline for Pretraining Cloth-Changing Person Re-Identification Models](https://arxiv.org/abs/2410.13567) \\
Yujian Zhao, Chengru Wu, Yinong Xu, Xuanzheng Du, Ruiyu Li, **Guanglin Niu**(Corresponding Author) \\
_IEEE International Conference on Multimedia & Expo (ICME), 2025_

- CCUP is a high-quality synthetic Cloth Changing Re-Identification (CC-ReID) dataset generated via a low-cost and controllable data generation pipeline, which is the first large-scale (over 1,000,000 images) dataset for the CC-ReID task.

üìÉ[**Paper**](https://arxiv.org/abs/2410.13567)     üíæ[**Code**](https://github.com/yjzhao1019/CCUP)
</div>
</div>


- ![CVPR Workshop 2025](https://img.shields.io/badge/CVPR_2025_Workshop-00369F) [Neighbor-Based Feature and Index Enhancement for Person Re-Identification](https://arxiv.org/abs/2504.11798), Chao Yuan, Tianyi Zhang, **Guanglin Niu**(Corresponding Author).
- ![ICME 2025](https://img.shields.io/badge/ICME_2025-00369F) [Knowledge Distilled Group Prompts Learning for HOI Detection with Large Vision-Language Models](https://arxiv.org/abs/2410.13567), Xiaoqian Han, **Guanglin Niu**, Mingliang Zhou, Xiaowei Zhang.
- ![Multimedia Systems 2024](https://img.shields.io/badge/Multimedia_Systems_2024-00369F) [Hierarchical bi-directional conceptual interaction for text-video retrieval](https://link.springer.com/article/10.1007/s00530-024-01525-3), Wenpeng Han, **Guanglin Niu**, Mingliang Zhou, Xiaowei Zhang.
- ![IEEE Signal Processing Letters 2024](https://img.shields.io/badge/IEEE_Signal_Processing_Letters_2024-00369F) [Geometry-Guided Point Generation for 3D Object Detection](https://ieeexplore.ieee.org/abstract/document/10758765), Kai Wang, Mingliang Zhou, Qing Lin, **Guanglin Niu**, Xiaowei Zhang.
- ![Neurocomputing 2019](https://img.shields.io/badge/Neurocomputing_2019-00369F) [Real-time object tracking via self-adaptive appearance modeling](https://www.sciencedirect.com/science/article/abs/pii/S092523121930548X?via%3Dihub), Ming Xin, Jin Zheng, Bo Li, **Guanglin Niu**, Miaohui Zhang.

  
## ![](https://img.shields.io/badge/-%2300A67E?style=flat-square&logo=OpenAI&logoColor=white) Large Language Model
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2025</div><img src='images/tablebench.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Tablebench: A comprehensive and complex benchmark for table question answering](https://arxiv.org/pdf/2408.09174) \\
Xianjie Wu, Jian Yang, Linzheng Chai, Ge Zhang, Jiaheng Liu, Xinrun Du, Di Liang, Daixin Shu, Xianfu Cheng, Tianzhen Sun, **Guanglin Niu**, Tongliang Li, Zhoujun Li \\
_Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2025_

- TableBench is a human-annotated comprehensive and complex TableQA benchmark comprising 886 samples across 18 fields, designed to facilitate fact-checking, numerical reasoning, data analysis, and visualization tasks.
- Our work is promoted by several media and forums, such as [AINLP](https://mp.weixin.qq.com/s/U9y5ncIcqS1Gt0S8Lx6ohw)

üîß[**Project**](https://tablebench.github.io//)     üìÉ[**Paper**](https://arxiv.org/pdf/2408.09174)     üíæ[**Code**](https://github.com/TableBench/TableBench) ![img](https://img.shields.io/github/stars/TableBench/TableBench?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2024</div><img src='images/mdeval.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MdEval: Massively Multilingual Code Debugging](https://arxiv.org/abs/2411.02310) \\
Shukai Liu, Linzheng Chai, Jian Yang, Jiajun Shi, He Zhu, Liran Wang, Ke Jin, Wei Zhang, Hualei Zhu, Shuyue Guo, Tao Sun, Jiaheng Liu, Yunlong Duan, Yu Hao, Liqun Yang, **Guanglin Niu**, Ge Zhang, Zhoujun Li

- MdEval is the first massively multilingual debugging benchmark, which includes 3.6K test samples of 18 programming languages and covers the automated program repair (APR) task, the code review (CR) task, and the bug identification (BI) task.

üìÉ[**Paper**](https://arxiv.org/pdf/2411.02310)     [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Datasets)](https://huggingface.co/datasets/Multilingual-Multimodal-NLP/MDEVAL)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2024</div><img src='images/fuzzcoder.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[FuzzCoder: Byte-level Fuzzing Test via Large Language Model](https://arxiv.org/abs/2409.01944) \\
Liqun Yang, Jian Yang, Chaoren Wei, **Guanglin Niu**, Ge Zhang, Yunli Wang, et al

- FuzzCoder formulates the fuzzing test as a sequenceto-sequence paradigm and then introduce the generation model to attack vulnerable positions by selecting proper mutation positions and strategies.

üìÉ[**Paper**](https://arxiv.org/pdf/2409.01944)     üíæ[**Code**](https://github.com/weimo3221/FUZZ-CODER) ![img](https://img.shields.io/github/stars/weimo3221/FUZZ-CODER?style=social)
</div>
</div>


## üåê Others
- ![AST 2018](https://img.shields.io/badge/AST_2018-00369F) [Spacecraft Attitude Fault-Tolerant Control Based on Iterative Learning Observer and Control Allocation](https://www.sciencedirect.com/science/article/abs/pii/S1270963817314906), Qinglei Hu, **Guanglin Niu**, Chenliang Wang <strong><span class='show_paper_citations' data='ibL7gEcAAAAJ:7PzlFSSx8tAC'></span></strong>
- ![ISA Transactions 2017](https://img.shields.io/badge/ISA_Transactions_2017-00369F) [Attitude Output Feedback Control for Rigid Spacecraft with Finite-Time Convergence](https://www.sciencedirect.com/science/article/abs/pii/S0019057817305074), Qinglei Hu, **Guanglin Niu**
- ![AST Transactions 2019](https://img.shields.io/badge/AST_2019-00369F) [Observer-Based Fault Tolerant Control and Experimental Verification for Rigid Spacecraft](https://www.sciencedirect.com/science/article/abs/pii/S1270963819308740), Qinglei Hu, Xinxin Zhang, **Guanglin Niu**
- ![CCC 2017](https://img.shields.io/badge/CCC_2017-00369F) [Iterative Disturbance Observer Design for Spacecraft Fault-Tolerant Control with Actuator Failure](https://ieeexplore.ieee.org/document/8028864), **Guanglin Niu**, Qinglei Hu, Lei Guo
- ![CCC 2016](https://img.shields.io/badge/CCC_2016-00369F) [Robust Finite-Time Observer Design for Rigid Spacecraft with Reaction Wheel Friction](https://ieeexplore.ieee.org/document/7555050), **Guanglin Niu**, Qinglei Hu
