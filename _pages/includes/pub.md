
# üìù Publications 
## Knowledge Graph


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">DASFAA 2025</div><img src='images/dhns.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Diffusion-based Hierarchical Negative Sampling for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2501.15393) \\
**Guanglin Niu**, Xiaowei Zhang

_The 30th International Conference on Database Systems for Advanced Applications (DASFAA), 2025_

- DHNS is the first to leverage the diffusion model‚Äôs capabilities within the context of multi-modal knowledge graph for negative sampling.

üìÉ[**Paper**](https://arxiv.org/pdf/2501.15393)     üíæ[**Code**](https://github.com/ngl567/DHNS) ![img](https://img.shields.io/github/stars/ngl567/DHNS?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2023</div><img src='images/lcge.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Logic and Commonsense-Guided Temporal Knowledge Graph Completion](https://ojs.aaai.org/index.php/AAAI/article/view/25579) \\
**Guanglin Niu**, Bo Li

_Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2023_

  - This work LCGE is the first to introduce temporal rules into temporal knowledge graph completion models.
  - LCGE models each event from the perspectives of both the time-sensitive representation and the commonsense.
  - Our work is promoted by some forums, such as [AI TimeÈùíÂπ¥ÁßëÂ≠¶ÂÆ∂ËÆ∫Âùõ](https://mp.weixin.qq.com/s/GP_S9U4EWJD0JGZcdJO3lg).

üìÉ[**Paper**](https://ojs.aaai.org/index.php/AAAI/article/view/25579)     üíæ[**Code**](https://github.com/ngl567/LCGE) ![img](https://img.shields.io/github/stars/ngl567/LCGE?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2022</div><img src='images/cake.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion](https://aclanthology.org/2022.acl-long.205) <strong><span class='show_paper_citations' data='ibL7gEcAAAAJ:3fE2CSJIrl8C'></span></strong> \\
**Guanglin Niu**, Bo Li, Yongfei Zhang, Shiliang Pu

_Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (ACL), 2022_

  - This work CAKE is the first to propose a scalable knowledge graph completion framework to predict entities in a joint commonsense and fact-driven fashion.
  - CAKE generates commonsense automatically for negative sampling and multi-view link prediction.
  - Our work is promoted by several media and forums, such as [AI Time ËßÜÈ¢ë](https://www.bilibili.com/video/BV1Q44y1g78Z/) \| [AI Time Ëß£ËØª](https://mp.weixin.qq.com/s/xQ625k_2kYXerZtO6M8mGg)„ÄÅ[‰∏ìÁü•](https://www.zhuanzhi.ai/document/5648511d67d6e512eb3521ac47d763d4)„ÄÅ[Êô∫Ê∫êÁ§æÂå∫](https://hub.baai.ac.cn/view/19366)„ÄÅ[ÂºÄÊîæÁü•ËØÜÂõæË∞±](https://mp.weixin.qq.com/s/1wVS2aJd6ddyPkvZHx3Lrw)„ÄÅ[AMiner](https://www.aminer.cn/research_report/627c81397cb68b460fb6063d).

üìÉ[**Paper**](https://aclanthology.org/2022.acl-long.205.pdf)     üíæ[**Code**](https://github.com/ngl567/CAKE) ![img](https://img.shields.io/github/stars/ngl567/CAKE?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">SIGIR 2021</div><img src='images/gana.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Relational Learning with Gated and Attentive Neighbor Aggregator for Few-Shot Knowledge Graph Completion](https://dl.acm.org/doi/10.1145/3404835.3462925) <strong><span class='show_paper_citations' data='ibL7gEcAAAAJ:WF5omc3nYNoC'></span></strong> \\
**Guanglin Niu**, Yang Li, Chengguang Tang, Ruiying Geng, Jian Dai, Qiao Liu, Hao Wang, Jian Sun, Fei Huang, Luo Si

_Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2021_

  - This approach GANA is the first to propose a gated and attentive neighbor aggregator to capture the most valuable contextual semantics of a relation.
  - GANA is one of the most representative models and always selected as the baseline on few-show knowledge graph completion tasks.
  - This work was conducted in collaboration with Qwen team. Our work is promoted by some media and forums, such as [‰∏ìÁü•](https://www.zhuanzhi.ai/document/01403034427fa0520e958ee1fe4afc56).

üìÉ[**Paper**](https://arxiv.org/pdf/2104.13095)     üíæ[**Code**](https://github.com/ngl567/GANA-FewShotKGC) ![img](https://img.shields.io/github/stars/ngl567/GANA-FewShotKGC?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2020</div><img src='images/rpje.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Rule-Guided Compositional Representation Learning on Knowledge Graphs](https://ojs.aaai.org//index.php/AAAI/article/view/5687)
<strong><span class='show_paper_citations' data='ibL7gEcAAAAJ:IjCSPb-OGe4C'></span></strong> \\
**Guanglin Niu**, Yongfei Zhang, Bo Li, Peng Cui, Si Liu, Jingyang Li and Xiaowei Zhang

_Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2020_

  - This work RPJE is the first attempt to integrate logic rules with paths for KG embedding, balancing the explainability and the generalization.
  - Our work is promoted by several media and forums, such as [ÂºÄÊîæÁü•ËØÜÂõæË∞±](https://mp.weixin.qq.com/s/tsXKwgbd2Z0XZcZZD2wcwQ)„ÄÅ[Èõ∑ÈîãÁΩë](https://www.leiphone.com/news/201912/5yfuCAlZlbFDypnH.html)„ÄÅ[SAAI](https://zhuanlan.zhihu.com/p/137519588)„ÄÅ[MLNLP](https://www.bilibili.com/video/BV1zV4y1V7j4/). Particularly, RPJE was recognized as one of the representative studies of neuro-symbolic KG reasoning at ([CCKS 2021](https://event-cdn.baai.ac.cn/live/20211228-01/Session4.mp4)).

üìÉ[**Paper**](https://ojs.aaai.org//index.php/AAAI/article/view/5687)     üíæ[**Code**](https://github.com/ngl567/RPJE) ![img](https://img.shields.io/github/stars/ngl567/RPJE?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2020 Findings</div><img src='images/autoeter.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[AutoETER: Automated Entity Type Representation with Relation-Aware Attention for Knowledge Graph Embedding](https://www.aclweb.org/anthology/2020.findings-emnlp.105/)
<strong><span class='show_paper_citations' data='ibL7gEcAAAAJ:IjCSPb-OGe4C'></span></strong> \\
**Guanglin Niu**, Yongfei Zhang, Bo Li, Shiliang Pu and Jingyang Li

_Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings (EMNLP Findings), 2020_

  - This work is the first to automatically learn the embeddings of entity types to enrich the general features of entities without explicit type information.
  - Our work is promoted by some media, such as [AI Time](https://www.bilibili.com/video/BV1Q44y1g78Z/)„ÄÅ[SFFAI](https://www.bilibili.com/video/av590645807/)„ÄÅ[‰∏ìÁü•](https://mp.weixin.qq.com/s/bU6Y42250GLmXiRzi8N1Wg).

üìÉ[**Paper**](https://aclanthology.org/2020.findings-emnlp.105.pdf)     üíæ[**Code**](https://github.com/ngl567/AutoETER) ![img](https://img.shields.io/github/stars/ngl567/AutoETER?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">COLING 2022</div><img src='images/enginekg.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Perform like an Engine: A Closed-Loop Neural-Symbolic Learning Framework for Knowledge Graph Inference](https://aclanthology.org/2022.coling-1.119/)\\
**Guanglin Niu**, Bo Li, Yongfei Zhang, Shiliang Pu

_Proceedings of the 29th International Conference on Computational Linguistics (COLING), 2022_

  - EngineKG performs like a four-stroke engine in a closed-loop neural-symbolic learning framework with embedding-based rule learning and rule-enhanced knowledge graph embedding. 
  - Our work is promoted by some media and forums, such as [MLNLP Talk](https://www.bilibili.com/video/BV1zV4y1V7j4/).

üìÉ[**Paper**](https://aclanthology.org/2022.coling-1.119.pdf)     üíæ[**Code**](https://github.com/ngl567/EngineKG)
</div>
</div>


- `ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ 2024` [Èù¢ÂêëÂÖ≥Á≥ªÁâπÊÄßÂª∫Ê®°ÁöÑÁü•ËØÜÂõæË∞±Ë°®Á§∫Â≠¶‰π†Á†îÁ©∂ÁªºËø∞](https://www.jsjkx.com/CN/article/openArticlePDF.jsp?id=22673), **Guanglin Niu**, Zhen Lin.
- ``Arxiv 2024`` [Knowledge Graph Embeddings: A Comprehensive Survey on Capturing Relation Properties](https://arxiv.org/pdf/2410.14733), **Guanglin Niu**. This paper is a modified English version of our article already published in Computer Science journal (in Chinese), released to facilitate communication among international researchers in the relevant fields.
- ``Arxiv 2024`` [A Pluggable Common Sense-Enhanced Framework for Knowledge Graph Completion](https://arxiv.org/pdf/2410.04488), **Guanglin Niu**, Bo Li, Siling Feng, et al.
- `Neurocomputing 2022` [Joint Semantics and Data-Driven Path Representation for Knowledge Graph Reasoning](https://www.sciencedirect.com/science/article/abs/pii/S0925231222001515), **Guanglin Niu**, Bo Li, Yongfei Zhang, et al.
- ``ACL 2021`` [Entity Concept Enhanced Few-Shot Relation Extraction](https://aclanthology.org/2021.acl-short.124/), Shan Yang, Yongfei Zhang, **Guanglin Niu**, Qinghua Zhao, et al. \| [![](https://img.shields.io/github/stars/LittleGuoKe/ConceptFERE?style=social&label=Code+Stars)](https://github.com/LittleGuoKe/ConceptFERE) <strong><span class='show_paper_citations' data='ibL7gEcAAAAJ:ufrVoPGSRksC'></span></strong>
- ``Arxiv 2021`` </span> [Path-Enhanced Multi-Relational Question Answering with Knowledge Graph Embeddings](https://arxiv.org/pdf/2110.15622.pdf), **Guanglin Niu**, Yang Li, Chengguang Tang, et al.


## üñºÔ∏è Computer Vision

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/camel.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CAMEL: CAusal Motion Enhancement tailored for Lifting Text-driven Video Editing](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_CAMEL_CAusal_Motion_Enhancement_Tailored_for_Lifting_Text-driven_Video_Editing_CVPR_2024_paper.html) \\
Guiwei Zhang, Tianyu Zhang, **Guanglin Niu**(Corresponding Author), Zichang Tan, Yalong Bai, Qing Yang

_Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024_

- DHNS develops a causal motion-enhanced attention mechanism to enhance the motion coherence of latent representations while preserving content generalization to creative textual scenarios.

üìÉ[**Paper**](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_CAMEL_CAusal_Motion_Enhancement_Tailored_for_Lifting_Text-driven_Video_Editing_CVPR_2024_paper.pdf)     üíæ[**Code**](https://github.com/zhangguiwei610/CAMEL) ![img](https://img.shields.io/github/stars/zhangguiwei610/CAMEL?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2025</div><img src='images/ifd.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Identity-aware Feature Decoupling Learning for Clothing-change Person Re-identification](https://arxiv.org/pdf/2501.05851) \\
Guiwei Zhang, Tianyu Zhang, **Guanglin Niu**(Corresponding Author), Zichang Tan, Yalong Bai, Qing Yang

_2025 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2025_

- IFD is the first to propose a dual-stream identity-attention model that effectively compels the network to focus comprehensively on the regions containing distinctive identity information.

üìÉ[**Paper**](https://arxiv.org/pdf/2501.05851)
</div>
</div>


- `Multimedia Systems 2024` [Hierarchical bi-directional conceptual interaction for text-video retrieval](https://link.springer.com/article/10.1007/s00530-024-01525-3), Wenpeng Han, **Guanglin Niu**, Mingliang Zhou, Xiaowei Zhang.
- `IEEE Signal Processing Letters 2024` [Geometry-Guided Point Generation for 3D Object Detection](https://link.springer.com/article/10.1007/s00530-024-01525-3), Kai Wang, Mingliang Zhou, Qing Lin, **Guanglin Niu**, Xiaowei Zhang.
- `IEEE Signal Processing Letters 2024` [CCUP: A Controllable Synthetic Data Generation Pipeline for Pretraining Cloth-Changing Person Re-Identification Models](https://link.springer.com/article/10.1007/s00530-024-01525-3), Yujian Zhao, Chengru Wu, Yinong Xu, Xuanzheng Du, Ruiyu Li, **Guanglin Niu**(Corresponding Author).
- `Neurocomputing 2019` [Real-time object tracking via self-adaptive appearance modeling](https://www.sciencedirect.com/science/article/abs/pii/S092523121930548X?via%3Dihub), Ming Xin, Jin Zheng, Bo Li, **Guanglin Niu**, Miaohui Zhang.

  
## üìö Large Language Model

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2025</div><img src='images/tablebench.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Tablebench: A comprehensive and complex benchmark for table question answering](https://arxiv.org/pdf/2408.09174) \\
Xianjie Wu, Jian Yang, Linzheng Chai, Ge Zhang, Jiaheng Liu, Xinrun Du, Di Liang, Daixin Shu, Xianfu Cheng, Tianzhen Sun, **Guanglin Niu**, Tongliang Li, Zhoujun Li 
_Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2025_

- TableBench is a human-annotated comprehensive and complex TableQA benchmark comprising 886 samples across 18 fields, designed to facilitate fact-checking, numerical reasoning, data analysis, and visualization tasks.
- Our work is promoted by several media and forums, such as [AINLP](https://mp.weixin.qq.com/s/U9y5ncIcqS1Gt0S8Lx6ohw)

[**Project**](https://tablebench.github.io//)     üìÉ[**Paper**](https://arxiv.org/pdf/2408.09174)     üíæ[**Code**](https://github.com/TableBench/TableBench) ![img](https://img.shields.io/github/stars/TableBench/TableBench?style=social)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2024</div><img src='images/mdeval.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MdEval: Massively Multilingual Code Debugging](https://arxiv.org/abs/2411.02310) \\
Shukai Liu, Linzheng Chai, Jian Yang, Jiajun Shi, He Zhu, Liran Wang, Ke Jin, Wei Zhang, Hualei Zhu, Shuyue Guo, Tao Sun, Jiaheng Liu, Yunlong Duan, Yu Hao, Liqun Yang, **Guanglin Niu**, Ge Zhang, Zhoujun Li

- MdEval is the first massively multilingual debugging benchmark, which includes 3.6K test samples of 18 programming languages and covers the automated program repair (APR) task, the code review (CR) task, and the bug identification (BI) task.

üìÉ[**Paper**](https://arxiv.org/pdf/2411.02310)     [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Datasets)](https://huggingface.co/datasets/Multilingual-Multimodal-NLP/MDEVAL)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2024</div><img src='images/fuzzcoder.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[FuzzCoder: Byte-level Fuzzing Test via Large Language Model](https://arxiv.org/abs/2409.01944) \\
Liqun Yang, Jian Yang, Chaoren Wei, **Guanglin Niu**, Ge Zhang, Yunli Wang, et al

- FuzzCoder formulates the fuzzing test as a sequenceto-sequence paradigm and then introduce the generation model to attack vulnerable positions by selecting proper mutation positions and strategies.

üìÉ[**Paper**](https://arxiv.org/pdf/2409.01944)     üíæ[**Code**](https://github.com/weimo3221/FUZZ-CODER) ![img](https://img.shields.io/github/stars/weimo3221/FUZZ-CODER?style=social)
</div>
</div>


## Others
- ``AST 2018`` [Spacecraft Attitude Fault-Tolerant Control Based on Iterative Learning Observer and Control Allocation](https://www.sciencedirect.com/science/article/abs/pii/S1270963817314906), Qinglei Hu, **Guanglin Niu**, Chenliang Wang
